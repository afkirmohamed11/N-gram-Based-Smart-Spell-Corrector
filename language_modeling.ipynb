{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f27b16f7-35b9-481f-b0e8-b36dd112e63b",
   "metadata": {},
   "source": [
    "## TP N°2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22934572-82f5-4e40-871b-a0019778453f",
   "metadata": {},
   "source": [
    "##### Ce travail est fait par AFKIR Mohamed et AKKOUH Lokmane"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b19fae9-b223-49d2-af1d-9c0cc5d74c78",
   "metadata": {},
   "source": [
    "L’objectif principal de cet exercice est d’implémenter des modèles de langue n-gramme et d'évaluer leurs performances, et ce, en utilisant les équations vues dans le cours. Ces modèles seront par la suite appliqués à la génération du texte, la correction automatique, et l'auto-complétion d’un texte."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0cd50938-fe82-4713-a8e8-6a52d11146ee",
   "metadata": {},
   "source": [
    "On commence tout d'abord par l'importation de toutes les bibliothèques necéssaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f0567c-d5be-447b-9692-b0aac3b56f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lokma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import conllu\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import json\n",
    "import string\n",
    "from help.emo_unicode import UNICODE_EMOJI_ALIAS as EMOTICONS ,UNICODE_EMOJI as UNICODE_EMO\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e90895-fd9b-4e01-90d1-a0378da3c9c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## I- Première Partie"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f1849c31-29b0-4fa4-a8ef-979de54bf989",
   "metadata": {},
   "source": [
    "On déclare à ce stade quelques fonctions utiles dont on aura besoin par la suite:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1b8dd-0519-4e97-bc5f-962cffd6466e",
   "metadata": {},
   "source": [
    "#### La méthode read_file(infile)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8f2a621-88a3-408a-9708-18cca958fd97",
   "metadata": {},
   "source": [
    "Cette méthode sert à lire un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a557276d-b436-4547-b8e6-5d49cc4dd5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(infile) -> str:\n",
    "    with open(infile, 'r', encoding='utf-8') as file:\n",
    "        corpus = file.read()\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a509d0-3fec-4d0d-8e19-7b5bfa04ec9b",
   "metadata": {},
   "source": [
    "#### La méthode vocabulary_size(infile) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cefd376-5b95-4306-bd6f-3b71ce0b483e",
   "metadata": {},
   "source": [
    "Cette méthode sert à retourner la taille d'un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14b0fb67-24f6-4fe3-bb81-a7138bf297fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary_size(file) -> int:\n",
    "    words = read_file(file).split(' ')\n",
    "    return len(set(words)) # La longueur de vocabulaire compris les symboles <s> et </s> car seront eventuellement pris en compte dans les calculs des probabilitées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf9d786-b0ae-439c-b770-86ca2ad67beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vocabulary_size(\"generated_files/ngramv1_train_processed_bigram.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f688d69-2b33-4ccc-af8d-78820c603524",
   "metadata": {},
   "source": [
    "#### La méthode len_of_sentence(sentence)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a670278-d15b-4f3d-b9db-ae6614ffb2c8",
   "metadata": {},
   "source": [
    "Cette fonction va nous permettre de calculer la taille d'une phrase donnée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee9e526-b5a9-4a9b-b118-44c72fad9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_of_sentence(sentence, corpus) -> int:\n",
    "    sentence = prepare_sentence(sentence, corpus)\n",
    "    return len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e07ff7-782a-4d1e-999a-c7d9cb7e87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len_of_sentence(\"hey, how are u !!\", \"data/ngramv1.train\")) #output = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad50ad-dd81-4704-8098-6d55ee586361",
   "metadata": {},
   "source": [
    "#### La méthode read_proba_file()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "395246f7-8db4-4971-b454-36df3e4edbdb",
   "metadata": {},
   "source": [
    "Cette méthode nous aidera par la suite pour lire le fichier de probabiltés( ce fichier sera généré pour qtocker les probabilités calculées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23ace8fe-114e-4ee5-a080-33f40a6c44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_proba_file(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "920571ce-6644-4182-b372-02924fe93b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(read_proba_file(\"generated_files/ngramv1_train_bigram_probas.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54962bcd-2189-47fe-b46f-a441ea7576b3",
   "metadata": {},
   "source": [
    "#### La méthode removes_urls_and_emails(text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "164cc651-5225-45e1-ba62-a1d09e188105",
   "metadata": {},
   "source": [
    "Cette méthode sert à supprimer les urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab5b312-92a7-49b9-9db6-e1fc2b8bafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls_and_emails(text):\n",
    "  url_pattern = r'(?:https?://)?(?:www\\.)?\\S+\\.([A-Za-z]|[A-Za-z0-9._%+-/])*'\n",
    "  clean_text = re.sub(url_pattern, '', text)  \n",
    "  email_pattern = r'\\b[A-Za-z0-9._%+-]+@([A-Za-z]|[A-Za-z0-9._%+-/])*'\n",
    "  clean_text = re.sub(email_pattern, '', clean_text)  \n",
    "  return clean_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5592d5",
   "metadata": {},
   "source": [
    "#### La méthode convert_emoticons(text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c47bf864-1c5d-4aad-9377-2a2756324601",
   "metadata": {},
   "source": [
    "Cette methode convertit les emojis en un texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a956fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emoticons(comment):\n",
    "    for emot1 in EMOTICONS:\n",
    "        pattern = re.escape(emot1)\n",
    "        comment = re.sub(pattern, ' emo_'+EMOTICONS[emot1][1:-1]+'_emo', comment)\n",
    "    for emot2 in UNICODE_EMO:\n",
    "        pattern = re.escape(emot2[0])\n",
    "        comment = re.sub(pattern, ' emo_'+re.sub(',','middcommamidd',re.sub(' ','_',emot2[1]))+'_emo', comment)\n",
    "    return str(comment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d4758",
   "metadata": {},
   "source": [
    "#### La methode convert_text_2_emoji()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16f79b3a-9741-4ece-93e1-0d7c6bd8d33c",
   "metadata": {},
   "source": [
    "Cette methode sert à convertir un texte en emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93cfe977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_2_emoji(comment):\n",
    "    for emot1 in EMOTICONS:\n",
    "        pattern = re.escape(emot1)\n",
    "        comment = re.sub(' emo_'+EMOTICONS[emot1][1:-1]+'_emo',pattern, comment)\n",
    "    for emot2 in UNICODE_EMO:\n",
    "        pattern = re.escape(emot2[0])\n",
    "        comment = re.sub(' emo_'+re.sub(',','middcommamidd',re.sub(' ','_',emot2[1]).lower())+'_emo',pattern, comment)\n",
    "    return str(comment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea77b8",
   "metadata": {},
   "source": [
    "#### La methode separate_emojies "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e293783-e9d7-4dad-b01e-820d765f06cd",
   "metadata": {},
   "source": [
    "Cette methode sert à séparer les emojis par un espace (s'ils sont liés)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee509ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_emojies(comment):\n",
    "    for emot1 in EMOTICONS:\n",
    "        pattern = re.escape(emot1)\n",
    "        comment = re.sub(pattern, ' '+emot1+' ', comment)\n",
    "    for emot2 in UNICODE_EMO:\n",
    "        pattern = re.escape(emot2[0])\n",
    "        comment = re.sub(pattern, ' '+str(emot2[0])+' ', comment)\n",
    "    return str(comment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce8a5e-79ea-466b-a55a-dfe5bbd10e99",
   "metadata": {},
   "source": [
    "#### La méthode add_space()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e43b077-fd4f-4984-941a-8b36ce2fc207",
   "metadata": {},
   "source": [
    "Cette méthode nous aidera à séparer les mots qui sont liés à des ponctuations (help_ devient help _)  et aussi les nombre comme #12 en # 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "422dd51f-66d7-4865-bedd-914e74cccd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_space(text):\n",
    "  \"\"\"Adds spaces around punctuations (except ', -, _, #') in a text.\n",
    "\n",
    "  Args:\n",
    "      text: The input text string.\n",
    "\n",
    "  Returns:\n",
    "      The modified text with spaces around punctuations.\n",
    "  \"\"\"\n",
    "  text=convert_emoticons(text)\n",
    "  text = text.lower()\n",
    "  sentences = text.split(\"\\n\")\n",
    "  punctuations = set(string.punctuation)\n",
    "  updated_sentences = []\n",
    "\n",
    "  for sentence in sentences:\n",
    "    updated_sentence = ''\n",
    "    for char in sentence:\n",
    "      if char in punctuations and char not in [\"'\", \"_\", \"-\", \"#\"]:\n",
    "        updated_sentence += ' ' + char + ' '\n",
    "      else:\n",
    "        updated_sentence += char\n",
    "    updated_sentence = re.sub(r'#(\\d+)', lambda match: '# ' + match.group(1), updated_sentence)\n",
    "    updated_sentences.append(updated_sentence)\n",
    "\n",
    "  updated_text = '\\n'.join(updated_sentences)\n",
    "  updated_text=convert_text_2_emoji(updated_text)\n",
    "  return updated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dadddaf-963c-4830-9ea7-9c7fcf044a87",
   "metadata": {},
   "source": [
    "#### La méthode prepare_sentence(infile) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d2255c1-6e08-486a-8f22-b30dd872fa51",
   "metadata": {},
   "source": [
    "Cette méthode sert à préparer une phrase afin de calculer sa probabilité (elle sera utilisée dans la troisième méthode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdae2943-1b36-497c-9168-8251304df80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(sentence, corpus, ngram_size = 2) -> str:\n",
    "    sentence = sentence.lower()\n",
    "    sentence = add_space(sentence) \n",
    "    sentence = remove_urls_and_emails(sentence)\n",
    "    sentence = separate_emojies(sentence)\n",
    "    \n",
    "    corpus = read_file(corpus)    \n",
    "    if ngram_size == 2:\n",
    "        sentence = ['<s> ' + sentence + ' </s>' ]\n",
    "    elif ngram_size == 3:\n",
    "        sentence = ['<s> <s> ' + sentence + ' </s>' ] # Pour le tri-gramme on ajoute deux <s><s> car on voit deux mots avant le mot cible\n",
    "\n",
    "    word_counts = defaultdict(int)\n",
    "    sentence = sentence[0].split()\n",
    "\n",
    "    for word in sentence:\n",
    "        word_counts[word] += 1\n",
    "    for i in  range(len(sentence)):\n",
    "        if (sentence[i] not in corpus):\n",
    "            sentence[i] = '<UNK>'\n",
    "    return ' '.join(sentence).strip().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "197ee93d-1a24-4639-b484-e97c63b96dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(prepare_sentence(\"i am ahmed .\", \"generated_files/part1/ngramv1_test_processed_bigram.txt\", ngram_size = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c39c5d1-dfc6-4106-b84e-42181bd0b3b9",
   "metadata": {},
   "source": [
    "#### La méthode prepare_data(infile, ngram_size=2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11d70012-0b09-4fd5-a6b5-1bedf25265d5",
   "metadata": {},
   "source": [
    "Cette méthode prend en entrée un fichier texte représentant le corpus, effectue la séparation des mots (tokenization), normalise le texte et ajoute les jetons de début et de fin de phrases. Cette méthode permet également de prendre en compte les mots (les tokens) hors vocabulaire. Pour ce faire, cette méthode recherche les mots qui apparaissent moins de N fois dans les données d'entraînement et elle les remplace par <UNK>. Cette méthode retourne le corpus prétraité sous forme d’une chaîne de caractères dont les tokens sont séparés par espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1c4aa24-a9a5-42fb-9560-0cdf212e90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(text,  ngram_size = 2, unk_threshold = 3 ) -> str:\n",
    "    \"\"\" \n",
    "    This method takes as input a text file representing the corpus \n",
    "    normalizes the text, and adds start  and end sentence tokens \n",
    "     Remember that you have to add a special '<s>' token to the beginning\n",
    "    and '</s>' token to the end of each sentence to correctly estimate the bigram\n",
    "    probabilities. \n",
    "    \n",
    "    Remember that you have to add a special '<s><s>' token to the beginning\n",
    "    and '</s>' token to the end of each sentence to correctly estimate the trigram\n",
    "    probabilities. \n",
    "    \n",
    "    This method also handles out-of-vocabulary words (tokens).\n",
    "    To achieve this, the method searches for words that appear less than N times in the training data and replaces them with <UNK>.\n",
    "    \n",
    "    Parameters\n",
    "    les_noms_des_tagsles_noms_des_tagsles_noms_des_tagsles_noms_des_tagsles_noms_des_tags\n",
    "    infile : str \n",
    "        File path to the training corpus.\n",
    "    \n",
    "    ngram_size : int\n",
    "        specifying which model to use. \n",
    "        Use this variable in an if/else statement. (n=2 for bigram and n=3 for trigram)\n",
    "    \n",
    "    Returns\n",
    "    les_noms_des_tagsles_noms_des_tagsles_noms_des_tags-\n",
    "    the preprocessed corpus\n",
    "    \n",
    "    \"\"\"\n",
    "    text = add_space(text) \n",
    "    text = remove_urls_and_emails(text)\n",
    "    text = separate_emojies(text)\n",
    "    \n",
    "    # text = convert_emoticons(text)\n",
    "    sentences = text.split(\"\\n\")\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        if ngram_size == 2:\n",
    "            sentences[i] = ['<s> ' + sentences[i] + ' </s>' ]\n",
    "        elif ngram_size == 3:\n",
    "            sentences[i] = ['<s> <s> ' + sentences[i] + ' </s>' ] # Pour le tri-gramme on ajoute deux <s><s> car on voit deux mots avant le mot cible\n",
    "    sentences=' '.join( [word for sublist in sentences for word in sublist])\n",
    "\n",
    "    word_counts = defaultdict(int) # Pour compter les occurrences des mots\n",
    "    sentences = sentences.strip().split() \n",
    "    for word in sentences:\n",
    "        word_counts[word] += 1\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        tokens = sentence.split()\n",
    "        sentences[i] = ' '.join(['<UNK>' if word_counts[word] < unk_threshold else word for word in tokens])\n",
    "    return ' '.join(sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d3b60abf-df62-476d-be1e-883a7d304d09",
   "metadata": {},
   "source": [
    "Nous stockons désormais les résultats du prétraitement dans des fichiers distincts pour les cas bigram et trigram. Cela nous permettra de minimiser le temps d'exécution des méthodes ultérieures, car au lieu de prétraiter le fichier à chaque appel de méthode, nous consulterons directement le fichier prétraité correspondant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dda347b-a907-4599-bec9-dc0a3a8a1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Les fichiers de partie 1\n",
    "# processed_text = prepare_data(read_file(\"data/part1/ngramv1.train\"), ngram_size = 2, unk_threshold=3)\n",
    "# with open(\"generated_files/part1/ngramv1_train_processed_bigram.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     file.write(processed_text)\n",
    "\n",
    "# processed_text = prepare_data(read_file(\"data/part1/ngramv1.train\"), ngram_size = 3, unk_threshold=3)\n",
    "# with open(\"generated_files/part1/ngramv1_train_processed_trigram.txt\", \"w\") as file:\n",
    "#     file.write(processed_text)\n",
    "\n",
    "\n",
    "# # Les fichiers de parie 2\n",
    "\n",
    "# processed_text = prepare_data(read_file('data/part2/big_data.txt'), ngram_size = 3, unk_threshold=3)\n",
    "# with open(\"generated_files/part2/big_data_processed_trigram.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     file.write(processed_text)\n",
    "\n",
    "# processed_text = prepare_data(read_file('data/part2/big_data.txt'), ngram_size = 2, unk_threshold=3)\n",
    "# with open(\"generated_files/part2/big_data_processed_bigram.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     file.write(processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1de665-368d-4ab2-90e7-efbe7ef13f25",
   "metadata": {},
   "source": [
    "#### La méthode Méthode train(ngram_size=2, infile)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "34ba4ccf-a97c-48bb-b977-a21e9985663d",
   "metadata": {},
   "source": [
    "Permet de calculer les probabilités des bigrammes et trigrammes. Le corpus à utiliser est donnée dans ngramv1.train. Utiliser la méthode prepare_data pour le prétraitement.\n",
    "Votre modèle doit prendre en compte les différents points étudiés dans le cours :\n",
    "- Prise en compte du traitement des mots hors vocabulaire.\n",
    "- Lissage (Utilisez le lissage add-k dans ce calcul).\n",
    "- Transformer les probabilités en logarithme comme discuté dans le cours pour éviter les problèmes de débordement des nombres flottants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f9c1145-6264-41e9-8285-d94cb22f58f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(infile, ngram_size = 2, k = 0.01) -> dict:\n",
    "    \"\"\"his method takes as input a text processed file representing the corpus\n",
    "       In addition of ngram_size and smoothinh factor k\n",
    "       Returns: a dictionnary of probability\n",
    "    \"\"\"\n",
    "    \n",
    "    data = read_file(infile)\n",
    "    data = data.strip().split(\" </s> \")\n",
    "    processed_data = []\n",
    "    for line in data[:-1]:\n",
    "        processed_data.append(line + \" </s>\")  \n",
    "    processed_data.append(data[-1]) \n",
    "    \n",
    "    #Initialiser les dictionnaires de probabilités\n",
    "    bigram_counts = defaultdict(int) \n",
    "    trigram_counts = defaultdict(int)\n",
    "    trigram_probs = defaultdict(int)\n",
    "    bigram_probs = defaultdict(int)\n",
    "    \n",
    "    # Initialiser un dictionnaire pour compter de mots uniques\n",
    "    global word_counts \n",
    "    word_counts = defaultdict(int)\n",
    "    \n",
    "    # Pour calculer les occurrences des couple de mots\n",
    "    global word_counts1 \n",
    "    word_counts1 = defaultdict(int) \n",
    "\n",
    "    # Récuperer la taille de vocabulaire\n",
    "    vocab_size = vocabulary_size(infile)\n",
    "    if ngram_size == 2:\n",
    "    # calculez les probabilités de bigrammes avec le lissage add-k\n",
    "        for sentence in processed_data:  \n",
    "            words = sentence.strip().split(\" \")  \n",
    "            for i in range(len(words) - 1):\n",
    "                pre_word, word = words[i], words[i + 1]\n",
    "                bigram_counts[(pre_word, word)] += 1\n",
    "            for w in words:\n",
    "                word_counts[w] += 1\n",
    "\n",
    "        for (pre_word, word) in list(bigram_counts.keys()):\n",
    "            count = bigram_counts[(pre_word, word)]\n",
    "            bigram_probs[(pre_word, word)] = (count + k) / (word_counts[pre_word] + vocab_size * k)\n",
    "\n",
    "        for (pre_word, word) in list(bigram_counts.keys()):\n",
    "                        prob = bigram_probs[(pre_word, word)]\n",
    "                        bigram_probs[(pre_word, word)] = np.log(prob) \n",
    "        return bigram_probs\n",
    "        \n",
    "    elif ngram_size == 3:\n",
    "        for sentence in processed_data:\n",
    "            words = sentence.strip().split(\" \")\n",
    "            for i in range(len(words) - 2):\n",
    "                pre_pre_word, pre_word, word = words[i], words[i + 1], words[i + 2]\n",
    "                trigram_counts[(pre_pre_word, pre_word, word)] += 1\n",
    "\n",
    "            for i in range(len(words)-1):\n",
    "                word_counts1[(words[i], words[i+1])] += 1\n",
    "        \n",
    "        # Calculer les probabilités du trigramme avec le lissage add-k\n",
    "        for (pre_pre_word, pre_word, word) in list(trigram_counts.keys()):\n",
    "            count = trigram_counts[(pre_pre_word, pre_word, word)]\n",
    "            trigram_probs[(pre_pre_word, pre_word, word)] = (count + k) / (word_counts1[(pre_pre_word, pre_word)] + vocab_size * k)\n",
    "\n",
    "        # Convertir les probabilités en log de probabilités\n",
    "        for (pre_pre_word, pre_word, word) in list(trigram_counts.keys()):\n",
    "            prob =  trigram_probs[(pre_pre_word, pre_word, word)]\n",
    "            trigram_probs[(pre_pre_word, pre_word, word)] = np.log(prob) \n",
    "        return trigram_probs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "616ba86e-e789-4873-bd89-92b5c9b2e3bb",
   "metadata": {},
   "source": [
    "Nous stockons les probabilités calculées dans un fichier .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1db279f1-3094-45e3-85cd-d0b4c8c0036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Les fichiers de partie 1\n",
    "# result1  = train(\"generated_files/part1/ngramv1_train_processed_trigram.txt\", ngram_size = 3, k=0.01)\n",
    "# # Convert tuple keys to string keys ( car on peut pas stocker les les tuples commes des clés dans un fichier .JSON)\n",
    "# result_str_keys = {str(key): value for key, value in result1.items()}\n",
    "# with open(\"generated_files/part1/ngramv1_train_trigram_probas.json\", \"w\") as json_file:\n",
    "#     json.dump(result_str_keys, json_file)\n",
    "\n",
    "\n",
    "# result1  = train(\"generated_files/part1/ngramv1_train_processed_bigram.txt\", ngram_size = 2, k=0.01)\n",
    "# # Convert tuple keys to string keys\n",
    "# result_str_keys2 = {str(key): value for key, value in result1.items()}\n",
    "# with open(\"generated_files/part1/ngramv1_train_bigram_probas.json\", \"w\") as json_file:\n",
    "#     json.dump(result_str_keys2, json_file)\n",
    "\n",
    "\n",
    "\n",
    "# # Les fichiers de partie 2\n",
    "# result1  = train(\"generated_files/part2/big_data_processed_trigram.txt\", ngram_size = 3, k=0.01)\n",
    "# # Convert tuple keys to string keys\n",
    "# result_str_keys = {str(key): value for key, value in result1.items()}\n",
    "# with open(\"generated_files/part2/big_data_trigram.json\", \"w\") as json_file:\n",
    "#     json.dump(result_str_keys, json_file)\n",
    "\n",
    "# result1  = train(\"generated_files/part2/big_data_processed_bigram.txt\", ngram_size = 2, k=0.01)\n",
    "# result_str_keys = {str(key): value for key, value in result1.items()}\n",
    "# with open(\"generated_files/part2/big_data_bigram.json\", \"w\") as json_file:\n",
    "#     json.dump(result_str_keys, json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f17fddb-b0cd-440a-b91f-1b307ec0aecb",
   "metadata": {},
   "source": [
    "#### La méthode predict_ngram(sentence, ngram_size = 2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "961db378-e478-4b88-9175-528eedbbbb59",
   "metadata": {},
   "source": [
    "Cette méthode prend en paramètre une phrase (sous forme de chaîne de caractères), utiliser la méthode prepare_data pour le prétraitement puis calcule la probabilité de la phrase en utilisant les modèles linguistique bigramme ou trigramme, selon la valeur d’un paramètre ngram_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eecda947-c399-455d-bead-5c1a2a4a6909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ngram(sentence, file, proba_file, ngram_size = 3, k = 0.01) -> float:\n",
    "    \"\"\"\n",
    "    Predicts the probability of a sentence using bigram or trigram model.\n",
    "\n",
    "    This function takes a sentence as input, preprocesses it using the prepare_sentence function,\n",
    "    and then calculates the probability of the sentence using the bigram or trigram probabilities\n",
    "    depending on the ngram_size parameter.\n",
    "\n",
    "    Args:\n",
    "        sentence: The sentence to predict the probability for (string).\n",
    "        file: file to get probabities.\n",
    "        ngram_size: Specifies the model to use (2 for bigram, 3 for trigram).\n",
    "        k : smoothing parameter.\n",
    "\n",
    "    Returns:\n",
    "        float: The log probability of the sentence.\n",
    "    \"\"\"\n",
    "    sentence  = prepare_sentence(sentence, file, ngram_size)\n",
    "    vocab_size = vocabulary_size(file)\n",
    "     \n",
    "    proba = 0.0\n",
    "    probas = read_proba_file(proba_file)\n",
    "\n",
    "    if ngram_size ==  2:\n",
    "        for j in range(len(sentence) - 1):\n",
    "            out = 0\n",
    "            pre_word, word = sentence[j], sentence[j+1] \n",
    "            for i in probas.keys():\n",
    "                l = i[2:-2]\n",
    "                l = l.split(\"', '\")                \n",
    "                if (pre_word, word) == (l[0],l[1]):\n",
    "                    proba += probas[i]\n",
    "                    out = 1\n",
    "            if out == 1:\n",
    "                continue\n",
    "            else:\n",
    "                proba += np.log(k / (word_counts[pre_word] + vocab_size * k))\n",
    "\n",
    "            \n",
    "\n",
    "    elif ngram_size == 3:\n",
    "        for j in range(len(sentence) - 2):\n",
    "            pre_pre_word, pre_word, word = sentence[j], sentence[j+1], sentence[j+2]  \n",
    "            out = 0\n",
    "            for i in probas.keys():\n",
    "                l = i[2:-2]\n",
    "                l = l.split(\"', '\")                \n",
    "                if (pre_pre_word,pre_word, word) == (l[0],l[1],l[2]):\n",
    "                    proba += probas[i]\n",
    "                    out = 1\n",
    "            if out == 1:\n",
    "                continue\n",
    "            else:\n",
    "                proba += np.log(k / (word_counts1[(pre_pre_word, pre_word)] + vocab_size * k))                \n",
    "    return proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7c44dbc-1fb5-46c6-9943-8a3ad18417e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predict_ngram(\"i am ahmed \", \"generated_files/part1/ngramv1_train_processed_bigram.txt\",\"generated_files/part1/ngramv1_train_trigram_probas.json\", ngram_size = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47616ff7-8595-4914-bf97-7b80f960c62c",
   "metadata": {},
   "source": [
    "#### La méthode test_perplexity(test_file, ngram_size = 2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "524ede4c-e33c-4e42-b397-61caa49bd0fa",
   "metadata": {},
   "source": [
    "Cette fonction prend le chemin d'un nouveau corpus (ngramv1.test) et calcule la perplexité par rapport à ce corpus de test. Et ce, en utilisant les modèles linguistique bigramme ou trigramme, selon la valeur du paramètre ngram_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26234397-20e3-481a-ab63-7d618a8441d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_perplexity(test_file, corpus, proba_file, ngram_size = 2) -> float:\n",
    "    \"\"\"Calculate the perplexity of the trained LM on a test corpus.\n",
    "\n",
    "        This seems complicated, but is actually quite simple. \n",
    "\n",
    "        First we need to calculate the total probability of the test corpus. \n",
    "        We can do this by summing the log probabiities of each sentence in the corpus.\n",
    "        \n",
    "        Then we need to normalize (e.g., divide) this summed log probability by the \n",
    "        total number of tokens in the test corpus. The one tricky bit here is we need\n",
    "        to augment this count of the total number of tokens by one for each sentence,\n",
    "        since we're including the sentence-end token in these probability estimates.\n",
    "\n",
    "        Finally, to convert this result back to a perplexity, we need to multiply it\n",
    "        by negative one, and exponentiate it - e.g., if we have the result of the above\n",
    "        in a variable called 'val', we will return math.exp(val). \n",
    "\n",
    "        Parameters\n",
    "        les_noms_des_tagsles_noms_des_tagsles_noms_des_tags-\n",
    "        test_file : str\n",
    "            File path to a test corpus.\n",
    "            (assumed pre-tokenized, whitespace-separated, one line per sentence)\n",
    "\n",
    "        ngram_size : int\n",
    "            specifying which model to use. \n",
    "            Use this variable in an if/else statement. (n=2 for bigram and n=3 for trigram)\n",
    "\n",
    "        Returns\n",
    "        les_noms_des_tagsles_noms_des_tagsles_noms_des_tags-\n",
    "        float  \n",
    "            The perplexity of the corpus (normalized total log probability).\n",
    "        \"\"\"\n",
    "    text = read_file(test_file)\n",
    "    sentences = text.split(\"\\n\")\n",
    "    tokens = prepare_data(text, ngram_size)\n",
    "    tokens = tokens.split(\" \")\n",
    "    \n",
    "    num_tokens = sum(1 for token in tokens if token != \"<s>\") # Nombre des tokens ( y compris le nombre de </s>)\n",
    "    \n",
    "    total_perplexity = 0.0\n",
    "    for sentence in sentences:\n",
    "        log_sentence_proba = -predict_ngram(sentence, corpus, proba_file, ngram_size)\n",
    "        total_perplexity += log_sentence_proba\n",
    "    return np.exp(total_perplexity / num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d46ba342-7927-41ea-a180-acc94b84f2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_perplexity(\"data/part1/ngramv1.test\", \"generated_files/part1/ngramv1_train_processed_trigram.txt\", \"generated_files/part1/ngramv1_train_trigram_probas.json\", ngram_size = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3467da11-409a-4fd5-b05f-c65a4899acd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## I- Deuxième Partie"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f39e47a9-b231-45db-9ca0-a28c5f0c888b",
   "metadata": {},
   "source": [
    "Dans cette partie nous allons utiliser de taille plus grande pour l’apprentissage et l’évaluation des modèles ngrammes (Utiliser les données dans big_data.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8205e1-b1f6-42b4-bbbc-2d647c9e3493",
   "metadata": {},
   "source": [
    "#### La méthode Méthode generateText() "
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d804484-597b-452f-b3a4-489fbc0f6604",
   "metadata": {},
   "source": [
    "Cette méthode permet de générer un texte en utilisant les modèles construits précédemment.\n",
    "Pour générer une phrase avec un modèle bigramme, par exemple, commencez par le jeton <s> et échantillonnez le mot suivant proportionnellement à sa probabilité de suivi de ce jeton. Vous pouvez utiliser, par exemple, la fonction numpy.random.choice. Par la suite, examinez tous les mots qui suivent ce mot, et échantillonnez à nouveau. Continuez l'échantillonnage jusqu'à ce que vous générer le jeton de fin de phrase </s>, moment auquel vous terminez la génération."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20cb6727-f61c-40d7-af63-217eed52cf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "global corpus\n",
    "corpus = \"data/part2/big_data.txt\" # On déclare à ce moment le corpus utilisé une fois pour toute\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23b267f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateText(proba_file, ngram_size = 2) -> str:\n",
    "    \"\"\"\n",
    "    Generates text using the trained bigram and trigram models.\n",
    "    \n",
    "    This method starts with a seed sentence (<s>) and iteratively samples the next word based on the probabilities\n",
    "    learned from the n-gram model. The process continues until the end-of-sentence token (</s>) is generated.\n",
    "    \n",
    "    Args:\n",
    "      proba_file : File that contains the ngram probabilities\n",
    "      ngram_size: Integer specifying the n-gram model to use (2 for bigram, 3 for trigram).\n",
    "      \n",
    "    \n",
    "    Returns:\n",
    "      str: The generated text.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    sentence = [\"<s>\"] * (ngram_size - 1)\n",
    "    probas = read_proba_file(proba_file)\n",
    "    while sentence[-1] != \"</s>\":\n",
    "      possible_words = []\n",
    "      probs = []\n",
    "      for tuple in probas.keys():\n",
    "        words = tuple[2:-2]\n",
    "        words = words.split(\"', '\")      \n",
    "        if words[ : ngram_size - 1] == sentence[-ngram_size + 1:]:\n",
    "          if (words[- 1] in ['<UNK>']):\n",
    "              pass\n",
    "          else:\n",
    "              probs.append(probas[tuple])\n",
    "              possible_words.append((probas[(tuple)], words[-1]))\n",
    "      \n",
    "\n",
    "      if (len(sentence)==(ngram_size-1)): # pour choisir un mot aleatoire au debut\n",
    "          probs = sorted(probs, reverse = True)[:20] # On donne la chance aux vingt  tokens les plus probables d'etre au debut de la phrase\n",
    "          f_proba=(np.random.choice(probs))\n",
    "          for p in possible_words :\n",
    "            if p[0] == f_proba :\n",
    "              sugg = p[1]\n",
    "          sentence.append(sugg)\n",
    "          continue\n",
    "      try:\n",
    "          probs = sorted(probs, reverse = True)[:5]\n",
    "          probability = np.random.choice(probs, size = 1)\n",
    "          for p in possible_words :\n",
    "              if p[0] == probability :\n",
    "                  sugg = p[1]\n",
    "          sentence.append(sugg)\n",
    "      except:\n",
    "        sentence.append('.')\n",
    "    return \" \".join(sentence[ngram_size-1:-1])  # Supprimer les jetons de début et de fin de phrase\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c125d861-5d8a-46f1-9491-1db7e080218c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just saw your heart attack on a lot . you are so i have been a lot , but i love that i have the world .\n",
      "___\n",
      "thanks to for a great day to my new favorite song ! barbie . . i love the new one in the morning show a fake id , his downfall ? wearing a \" closer \" then what ? !\n"
     ]
    }
   ],
   "source": [
    "# for i in range(5)=:\n",
    "print(generateText(\"generated_files/part2/big_data_bigram.json\", ngram_size = 2))\n",
    "print(\"___\")\n",
    "# for i in range(5):\n",
    "print(generateText(\"generated_files/part2/big_data_trigram.json\", ngram_size = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a0008-0bdb-4ec4-a597-eff12af555a9",
   "metadata": {},
   "source": [
    "#### La méthode autoComplete(text)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e90001ad-f1b4-4a55-9df4-faa39c26386b",
   "metadata": {},
   "source": [
    "Cette méthode nous permet de prédire le mot suivant le plus probable en se basant sur un historique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25b27149-c9db-480d-8797-67a310d1ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoComplete(sentence, proba_file,ngram_size = 2) -> str:\n",
    "    \"\"\"\n",
    "    Predicts next token based on previous ones.\n",
    "    \n",
    "    Args:\n",
    "      sentence: represents previous tokens\n",
    "      proba_file : File that contains the ngram probabilities\n",
    "    \n",
    "    Returns:\n",
    "      str: The predicted token plus previous sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    probas = read_proba_file(proba_file)\n",
    "    possible_words = []\n",
    "    probs = []\n",
    "    sentence = prepare_sentence(sentence,corpus)[1:-1]\n",
    "    \n",
    "    for tupl in probas.keys():\n",
    "        words = tupl[2:-2]\n",
    "        words = words.split(\"', '\")\n",
    "        for i in range(ngram_size - 1):\n",
    "            if words[ : ngram_size - 1] == sentence[-ngram_size + 1:]:\n",
    "                if (words[- 1] == '<UNK>' or words[- 1] == '</s>' ):\n",
    "                    pass\n",
    "                    \n",
    "                else:\n",
    "                    probs.append(probas[tupl])\n",
    "                    possible_words.append((probas[(tupl)], words[-1]))\n",
    "    if probs:\n",
    "        probs = sorted(probs, reverse = True)[:3]\n",
    "        f_proba=(np.random.choice(probs))\n",
    "        for p in possible_words : \n",
    "            if p[0] == f_proba :\n",
    "                suggestion = p[1]\n",
    "        sentence.append(suggestion)  \n",
    "        \n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "832d8b00-aecb-4218-821c-f20c81404dc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(autoComplete(\"he is here \",proba_file='generated_files/part2/big_data_bigram.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059cfba3-9547-4220-afcf-d9b9a380f1ab",
   "metadata": {},
   "source": [
    "#### La méthode correction()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21ba0e22-d54d-4f8b-ac97-9ae921ea51c5",
   "metadata": {},
   "source": [
    "Implémente une variante du correcteur d’orthographe réalisé dans le TP1 en utilisant un modèle de langue bigramme pour ordonner les corrections possibles par leurs probabilités selon le modèle de langue et selon la distance minimale d'édition."
   ]
  },
  {
   "cell_type": "raw",
   "id": "291cc660-0f41-4e5c-995b-dd2248945c7b",
   "metadata": {},
   "source": [
    "On importe dans cette section les fonctions utilisés dans le TP01 pour qui sert à corriger les mots incorrecte (ces fonctiosn sont présentes dans le fichier help/NLP_utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b8994a6-6438-422a-923f-b42624cd2703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from help.NLP_utils import candidates, get_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c15b695b-40c5-42b2-9aa5-fbdb220aee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(twput, proba_file, ngram_size = 2) -> str :\n",
    "    \"\"\"\n",
    "    Takes a text and corrects it.\n",
    "    \n",
    "    Args:\n",
    "      twput : Text with probable uncorrected tokens\n",
    "      proba_file : File that contains the ngram probabilities\n",
    "      ngram_size: Integer specifying the n-gram model to use (2 for bigram, 3 for trigram).\n",
    "      \n",
    "    \n",
    "    Returns:\n",
    "      str: The generated text.\n",
    "    \"\"\"    \n",
    "    probas = read_proba_file(proba_file)\n",
    "    C_words_List=get_vocabulary(read_file(corpus))                    \n",
    "    C_words=read_file(corpus)\n",
    "    probs=[]                                       \n",
    "    suggestions=[]  \n",
    "    twput = twput.lower()\n",
    "    twput = re.sub(r'[^\\w]',' ', twput) \n",
    "    twput = re.findall(r'\\w+', twput) \n",
    "    for i in range(ngram_size - 1):\n",
    "        twput.insert(i, \"<s>\")\n",
    "    twput.append(\"</s>\") # \"I like football\" les_noms_des_tags-> ['<s>', '<s>', 'I', 'like', 'football', '</s>'] if ngram_size = 3\n",
    "    for i in range(len(twput)):\n",
    "        probs=[]                                        #list of probabilities\n",
    "        suggestions=[]                                  #list of suggestions that are in our vocabulary and are provided by candidates\n",
    "        possible_words = []                             #list of words returned by candidates cotaining possible words after min edit\n",
    "        if twput[i] in [\"<s>\", \"</s>\"] or twput[i] in C_words_List :\n",
    "            pass\n",
    "        else:\n",
    "            possible_words.append(candidates(twput[i],corpus=C_words))\n",
    "            for sugg in possible_words[0]:\n",
    "                for tuple in probas.keys() :\n",
    "                    words = tuple[2:-2]\n",
    "                    words = words.split(\"', '\")\n",
    "                    if ((words[-1]==sugg) and (words[ : ngram_size - 1]==twput[i-ngram_size+1:i])):\n",
    "                        probs.append(probas[tuple])\n",
    "                        suggestions.append((probas[(tuple)], words[-1]))\n",
    "            if probs:\n",
    "                probs = sorted(probs, reverse = True)\n",
    "                for p in suggestions : \n",
    "                    if p[0] == probs[0] :\n",
    "                        suggestion = p[1]\n",
    "                        twput[i]=suggestion\n",
    "            else:\n",
    "                twput[i]=np.random.choice(possible_words[0])\n",
    "                    \n",
    "        \n",
    "    \n",
    "    return \" \".join(twput[ngram_size-1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ead770b2-3e5a-4ed6-8674-938137d91a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(correction(\"helo my frind \", proba_file='generated_files/part2/big_data_bigram.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fdbd44-d9f0-4e61-b63b-d8fcb53ec8c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## III- Troisième Partie"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f417c95a-aeea-431a-81f3-39e0f7452ffe",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "En utilisant le même algorithme implémenté dans le cours (avec éventuellement quelques adaptions) réaliser un système d’étiquetage morphosyntaxique pour la langue française en utilisant des données annotées de votre choix ou les données indiquées dans le lien suivant:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaab7c4e-d44e-4cf1-816f-547b7ca0278e",
   "metadata": {},
   "source": [
    "[https://github.com/qanastek/ANTILLES/tree/main/ANTILLES](https://github.com/qanastek/ANTILLES/tree/main/ANTILLES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc4018b",
   "metadata": {},
   "source": [
    "#### La méthode conllu_to_csv()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d90bde6e-ec24-4d01-a877-61c43783b0e0",
   "metadata": {},
   "source": [
    "Cette méthode nous aidera à transforMer le fichier au FORMat CoNLL-U en FORMat CSV en sélectionnant uniquement les colonnes 'FORM' et 'UPOS', car nous aurons seulement besoin de ces deux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b862d686-1c1a-4954-bf73-050e592ab846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conllu_to_csv(conllu_file, csv_file) -> None:\n",
    "    \"\"\"\n",
    "    This method takes a CoNLL-U format file and transform it to a csv one.\n",
    "    \"\"\"\n",
    "    with open(conllu_file, 'r', encoding='utf-8') as conllu, open(csv_file, 'w', newline='', encoding='utf-8') as csvout:\n",
    "        csv_writer = csv.writer(csvout)\n",
    "        csv_writer.writerow(['FORM', 'UPOS'])  # Only write FORM and UPOS headers\n",
    "\n",
    "        for line in conllu:\n",
    "            line = line.strip()\n",
    "            if line.startswith('#') or line == '':\n",
    "                continue\n",
    "\n",
    "            parts = line.split('\\t')\n",
    "            FORM = parts[1]  # FORM field\n",
    "            upos = parts[3]  # UPOS field\n",
    "\n",
    "            csv_writer.writerow([FORM,upos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a05d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "conllu_to_csv('data/part3/dev.conllu', 'data/part3/dev.csv')\n",
    "conllu_to_csv('data/part3/train.conllu', 'data/part3/train.csv')\n",
    "conllu_to_csv('data/part3/test.conllu', 'data/part3/test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ea4396",
   "metadata": {},
   "source": [
    "#### La méthode transition_probability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b142d28b",
   "metadata": {},
   "source": [
    "Cette méthode sert à construire la matrice de transition du modèle de POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eea39021-fc29-48fa-b7eb-47eaab407ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\lokma\\AppData\\Local\\Temp\\ipykernel_25840\\2071809288.py:16: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  num_points = df['FORM'].str.count('\\.').sum()\n"
     ]
    }
   ],
   "source": [
    "def transition_probability(train_file):\n",
    "    df = pd.read_csv(train_file)\n",
    "    unique_upos_values = df['UPOS'].unique()\n",
    "    unique_upos_values = np.concatenate([[\"les_noms_des_tags\"], unique_upos_values])\n",
    "    new_df = pd.DataFrame(columns=unique_upos_values)\n",
    "    # matrix columns construction\n",
    "    for upos_value in unique_upos_values:\n",
    "        if upos_value == \"les_noms_des_tags\":\n",
    "            new_df[upos_value] = unique_upos_values\n",
    "        else:\n",
    "            new_df[upos_value] = 0.0\n",
    "    new_df.iloc[0, 0] = '<s>'\n",
    "\n",
    "    # Count the number of sentences\n",
    "    num_points = df['FORM'].str.count('\\.').sum()\n",
    "\n",
    "    for col in new_df.columns[1:]:  # Exclude the first column 'les_noms_des_tags'\n",
    "        count = 0\n",
    "        if col == df.iloc[0,1]:\n",
    "            count += 1\n",
    "            num_points +=1\n",
    "        for i in range(len(df) - 1):\n",
    "            if df['FORM'][i] == \".\" and df['UPOS'][i + 1] == col:\n",
    "                count += 1\n",
    "            \n",
    "        new_df.loc[0, col] = count / num_points\n",
    "    \n",
    "    columns= new_df.columns[1:]\n",
    "    \n",
    "    for i in range(1,len(columns)+1):\n",
    "        for j in range(0,len(columns)):\n",
    "            count = 0\n",
    "            total_occurrences = df['UPOS'].value_counts()[columns[j]]\n",
    "            if total_occurrences == 0:  # Handle division by zero\n",
    "                new_df.iloc[j, i] = 0\n",
    "\n",
    "            else:\n",
    "                for k in range(len(df)-1):\n",
    "                    if df['UPOS'][k] == columns[j] and df['UPOS'][k+1] == columns[i-1]:\n",
    "                        count += 1\n",
    "                new_df.iloc[j+1, i] = count / total_occurrences\n",
    "               \n",
    " \n",
    "    print(new_df) \n",
    "            \n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1acfb-8e88-4b68-8229-4606d60e4c76",
   "metadata": {},
   "source": [
    "#### La méthode emission_probability()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40ca544a-265d-4f9a-a877-992e488eca24",
   "metadata": {},
   "source": [
    "Cette méthode sert à construire la matrice d'émission du modèle de POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37fc4448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emission_probability(train_file):\n",
    "  \"\"\"\n",
    "  Calculates emission probabilities for each word-tag pair in a sentence.\n",
    "\n",
    "  Args:\n",
    "      train_file: CSV file containing the training data.\n",
    "\n",
    "  Returns:\n",
    "      pd.DataFrame: A DataFrame holding the emission probabilities for the corpus.\n",
    "  \"\"\"\n",
    "\n",
    "  df = pd.read_csv(train_file)\n",
    "  unique_words = df['FORM'].unique()\n",
    "  unique_word = np.concatenate([[\"les_noms_des_tags\"], unique_words])\n",
    "  unique_upos_values = df['UPOS'].unique()\n",
    "\n",
    "  emission_df = pd.DataFrame(columns=unique_word)\n",
    "  emission_df = emission_df[['les_noms_des_tags'] + list(unique_words)]\n",
    "  emission_df[\"les_noms_des_tags\"] = unique_upos_values\n",
    "  emission_df.infer_objects(copy=False)\n",
    "  emission_df.fillna(0, inplace=True)\n",
    "  tags = {key: 0 for key in emission_df[\"les_noms_des_tags\"].tolist()}\n",
    "\n",
    "  for i in range(len(df)):\n",
    "      tag = df['UPOS'][i]\n",
    "      token = df['FORM'][i]\n",
    "      tags[tag] += 1\n",
    "      emission_df.loc[emission_df['les_noms_des_tags'] == tag, token] += 1\n",
    "  for i, tag in enumerate(unique_upos_values):\n",
    "      emission_df.iloc[i, 1:] = emission_df.iloc[i, 1:] / tags[tag]\n",
    "  return emission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d297224",
   "metadata": {},
   "source": [
    "#### La méthode viterbi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c2661",
   "metadata": {},
   "source": [
    "Cette méthode implémente l'algorithme de Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f6309c26-d1c2-4523-a3a4-ed3f78e83bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sentence,em_m,tr_m):\n",
    "    sentence=sentence.split()\n",
    "    sentence.insert(0,'<s>')\n",
    "    tags=[]    \n",
    "    vit_m=pd.DataFrame(columns=sentence[::])  # viterbi matrix\n",
    "    # vit_m.insert(0, 'les_noms_des_tags', tr_m.columns[1:])      # inserting a column of tags \n",
    "    row = tr_m.loc[tr_m['les_noms_des_tags'] == '<s>']          # getting the raw where 'les_noms_des_tags'is '<s>' from emition matrix                        \n",
    "    vit_m.insert(0, 'les_noms_des_tags', tr_m.columns[1:])      # inserting a column of tags\n",
    "    vit_m['<s>']=row.values.tolist()[0][1:]                    # getting values of tags /<s>\n",
    "    sentence=sentence[1:]                          # removing <s> and </s>\n",
    "    tags.append('<s>')\n",
    "    for j in range(1,len(sentence)+1):\n",
    "        i=0\n",
    "        found=False\n",
    "        for c in em_m.columns:                                  # loop on emition columns that are 'words'\n",
    "            if sentence[j-1]==c:\n",
    "                found=True\n",
    "                em=em_m[['les_noms_des_tags',sentence[j-1]]]        # getting the column of the current word from sentence\n",
    "                row = tr_m.loc[tr_m['les_noms_des_tags'] ==tags[j-1] ]      # getting the row where the tag is the previos tag from sentence (already saved in tags)\n",
    "                for c2 in row.columns[1:]:  \n",
    "                    word_tag_proba=em.loc[em['les_noms_des_tags']==c2].values.tolist()[0][1]        # probability of current_word/tag \n",
    "                    tag_from_em=em.loc[em['les_noms_des_tags']==c2].values.tolist()[0][0]           # the tag\n",
    "                    max_previous_column=max(vit_m.iloc[:,j])\n",
    "                    if c2==tag_from_em:\n",
    "                        new_value= row[c2].values.tolist()[0]*word_tag_proba*max_previous_column\n",
    "                        row_index = np.where(vit_m['les_noms_des_tags'] == c2)[0][0]    # index of current row\n",
    "                        vit_m.iloc[row_index, j+1] = new_value  # in the vit matrix we multipy the probs of current_word/tag with tag/tag\n",
    "                max_value_row_index=np.where(vit_m.iloc[:,j+1] ==max(vit_m.iloc[:,j+1] ))[0][0]\n",
    "                tags.append(vit_m.iloc[max_value_row_index,0])\n",
    "                i+=1\n",
    "                break\n",
    "            \n",
    "        if found==False:\n",
    "            for c2 in row.columns[1:]:\n",
    "                max_previous_column=max(vit_m.iloc[:,j]) \n",
    "                new_value= row[c2].values.tolist()[0]*max_previous_column\n",
    "                row_index = np.where(vit_m['les_noms_des_tags'] == c2)[0][0]    # index of current row\n",
    "                vit_m.iloc[row_index, j+1] = new_value  # in the vit matrix we multipy the probs of current_word/tag with tag/tag\n",
    "                max_value_row_index=np.where(vit_m.iloc[:,j+1] ==max(vit_m.iloc[:,j+1] ))[0][0]\n",
    "            tags.append(vit_m.iloc[max_value_row_index,0])\n",
    "            i+=1\n",
    "    return tags[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4da7116",
   "metadata": {},
   "source": [
    "#### La méthode test_viterbi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dba06d",
   "metadata": {},
   "source": [
    "Cette méthode sert à tester l'algorithme de Viterbi sur les données de validation premièrement et par quite sur les données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1a899333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_viterbi(test_file,em_m,tr_m):\n",
    "    data = pd.read_csv(test_file)\n",
    "    tags=data['UPOS'].tolist()\n",
    "    tags = [item for item in tags if item != 'YPFOR']\n",
    "    words = data['FORM']\n",
    "    words = \" \".join(words).lower()\n",
    "    count=0\n",
    "    sentences=words.split(\" . \")\n",
    "    pos=[]\n",
    "    for sentence in sentences:   \n",
    "        pos.append(viterbi(sentence,em_m=em_m,tr_m=tr_m))\n",
    "    pos=[item for sublist in pos for item in sublist]\n",
    "    for i in range(len(tags)):\n",
    "            if pos[i]==tags[i]:\n",
    "                count+=1\n",
    "    return count/len(tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "20052b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7118126272912424\n"
     ]
    }
   ],
   "source": [
    "\n",
    "em_m=pd.read_csv('generated_files/part3/emission_sample.csv')\n",
    "tr_m=pd.read_csv('generated_files/part3/transition_sample.csv')\n",
    "\n",
    "print(test_viterbi('data/part3/train_sample.csv',em_m=em_m,tr_m=tr_m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013941bb",
   "metadata": {},
   "source": [
    "#### Veuillez noter, que le modèle a été entraîné uniquement sur les données de développement, car nous n'avons pas pu construire la matrice d'émission pour les données d'entraînement. Cela a pris beaucoup de temps. Nous avons ensuite testé le modèle sur les données de test, où il a obtenu un score de 47%. Ceci est logique car il n'a pas été entraîné sur un grand ensemble de données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c40780",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
